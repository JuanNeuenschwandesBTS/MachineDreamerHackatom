{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have a DataFrame indexed by datetime, with columns for end time, unit name, PSR type, and quantity. The goal is to forecast the 'surplus' and develop an ARIMA model for that. Let's recap on the steps we need for this goal:\n",
    "\n",
    "### Steps for Forecasting\n",
    "\n",
    "#### 1. Data Preprocessing (Done in Data Wrangling)\n",
    "- **Convert 'EndTime' to DateTime**: Convert the 'StartTime' column to a datetime object.\n",
    "- **Set 'EndTime' as Index**: Set 'EndTime' as the DataFrame index for easier time series analysis.\n",
    "- **Check for Missing Dates**: Ensure that all dates and times are represented. If there are missing intervals, decide how to handle them (e.g., filling with NaNs).\n",
    "- **Aggregating Data**: Depending on the granularity you need, you might consider aggregating your data (e.g., daily average). In this case, the resample is done by hour. \n",
    "\n",
    "#### `2. Exploratory Data Analysis (In this notebook)`\n",
    "- **Plotting the Series**: Visualize the 'surplus' over time to understand patterns, trends, and seasonality.\n",
    "- **Seasonality and Trend Analysis**: Check if there is any visible seasonality or trend in the data which might influence the choice of the model.\n",
    "\n",
    "#### 3. Model Selection\n",
    "Given the nature of your data, you might consider the following models:\n",
    "- **ARIMA/SARIMA**: If your data shows trends or autocorrelation. SARIMA is suitable if there is a clear seasonal pattern.\n",
    "- **Prophet**: Handles daily data well, robust to missing data, and good with seasonality.\n",
    "- **Machine Learning Approaches**: If there are other factors that can predict 'quantity', a model like Random Forest or Gradient Boosting might be useful.\n",
    "\n",
    "#### 4. Model Training and Forecasting\n",
    "- **Training**: Use data from January to April and October to December.\n",
    "- **Forecasting**: Generate predictions for the missing months.\n",
    "- **Hyperparameter Tuning**: Optimize model parameters for best performance.\n",
    "\n",
    "#### 5. Model Evaluation\n",
    "- **Performance Metrics**: Evaluate the model using appropriate metrics.\n",
    "- **Cross-Validation**: If possible, use time series cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import numpy as np\n",
    "#import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller#, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Magic commands \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data Wrangling output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing date strings, ignoring any timezone information and converting them to datetime objects\n",
    "date_parser = lambda x: pd.to_datetime(x[:22])\n",
    "eda_df = pd.read_csv(\"../2_data_wrangling/data_wrangling_output.csv\", \n",
    "                     converters={'EndTime': date_parser}).set_index('EndTime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>quantity_sum</th>\n",
       "      <th>Load</th>\n",
       "      <th>surplus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EndTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00+00:00</th>\n",
       "      <td>DE</td>\n",
       "      <td>146300.0</td>\n",
       "      <td>166143</td>\n",
       "      <td>-19843.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00+00:00</th>\n",
       "      <td>DE</td>\n",
       "      <td>140324.0</td>\n",
       "      <td>161923</td>\n",
       "      <td>-21599.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00+00:00</th>\n",
       "      <td>DE</td>\n",
       "      <td>134063.0</td>\n",
       "      <td>158256</td>\n",
       "      <td>-24193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00+00:00</th>\n",
       "      <td>DE</td>\n",
       "      <td>128745.0</td>\n",
       "      <td>157353</td>\n",
       "      <td>-28608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00+00:00</th>\n",
       "      <td>DE</td>\n",
       "      <td>120346.0</td>\n",
       "      <td>155306</td>\n",
       "      <td>-34960.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          country_code  quantity_sum    Load  surplus\n",
       "EndTime                                                              \n",
       "2022-01-01 00:00:00+00:00           DE      146300.0  166143 -19843.0\n",
       "2022-01-01 01:00:00+00:00           DE      140324.0  161923 -21599.0\n",
       "2022-01-01 02:00:00+00:00           DE      134063.0  158256 -24193.0\n",
       "2022-01-01 03:00:00+00:00           DE      128745.0  157353 -28608.0\n",
       "2022-01-01 04:00:00+00:00           DE      120346.0  155306 -34960.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 67889 entries, 2022-01-01 00:00:00+00:00 to 2022-12-31 23:00:00+00:00\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   country_code  67889 non-null  object \n",
      " 1   quantity_sum  67889 non-null  float64\n",
      " 2   Load          67889 non-null  int64  \n",
      " 3   surplus       67889 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "eda_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable Name | Description                            | Non-Null Count | Data Type |\n",
    "|---------------|----------------------------------------|----------------|-----------|\n",
    "| **`Index`**     | Date and time (hourly)                 | 67889          | datetime  |\n",
    "| `country_code`  | Identifier for the country             | 67889          | object    |\n",
    "| `quantity_sum`  | Sum of quantities                      | 67889          | float64   |\n",
    "| `Load`          | Load value                             | 67889          | int64     |\n",
    "| `surplus`       | Energy surplus value                          | 67889          | float64   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON of country codes for mapping\n",
    "countries_dict = {\n",
    "    \"SP\": 0,  # Spain\n",
    "    \"UK\": 1,  # United Kingdom\n",
    "    \"DE\": 2,  # Germany\n",
    "    \"DK\": 3,  # Denmark\n",
    "    \"HU\": 5,  # Hungary\n",
    "    \"SE\": 4,  # Sweden\n",
    "    \"IT\": 6,  # Italy\n",
    "    \"PO\": 7,  # Poland\n",
    "    \"NE\": 8   # Netherlands\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DE', 'NE', 'DK', 'HU', 'SP', 'IT', 'SE', 'UK', 'PO'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_df['country_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the energy type codes to full names\n",
    "eda_df['CountryLabel'] = eda_df['country_code'].map(countries_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 67889 entries, 2022-01-01 00:00:00+00:00 to 2022-12-31 23:00:00+00:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   country_code  67889 non-null  object \n",
      " 1   quantity_sum  67889 non-null  float64\n",
      " 2   Load          67889 non-null  int64  \n",
      " 3   surplus       67889 non-null  float64\n",
      " 4   CountryLabel  67889 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(1)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "eda_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>quantity_sum</th>\n",
       "      <th>Load</th>\n",
       "      <th>surplus</th>\n",
       "      <th>CountryLabel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EndTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00+00:00</th>\n",
       "      <td>DE</td>\n",
       "      <td>146300.0</td>\n",
       "      <td>166143</td>\n",
       "      <td>-19843.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00+00:00</th>\n",
       "      <td>DE</td>\n",
       "      <td>140324.0</td>\n",
       "      <td>161923</td>\n",
       "      <td>-21599.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00+00:00</th>\n",
       "      <td>DE</td>\n",
       "      <td>134063.0</td>\n",
       "      <td>158256</td>\n",
       "      <td>-24193.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00+00:00</th>\n",
       "      <td>DE</td>\n",
       "      <td>128745.0</td>\n",
       "      <td>157353</td>\n",
       "      <td>-28608.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00+00:00</th>\n",
       "      <td>DE</td>\n",
       "      <td>120346.0</td>\n",
       "      <td>155306</td>\n",
       "      <td>-34960.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          country_code  quantity_sum    Load  surplus  \\\n",
       "EndTime                                                                 \n",
       "2022-01-01 00:00:00+00:00           DE      146300.0  166143 -19843.0   \n",
       "2022-01-01 01:00:00+00:00           DE      140324.0  161923 -21599.0   \n",
       "2022-01-01 02:00:00+00:00           DE      134063.0  158256 -24193.0   \n",
       "2022-01-01 03:00:00+00:00           DE      128745.0  157353 -28608.0   \n",
       "2022-01-01 04:00:00+00:00           DE      120346.0  155306 -34960.0   \n",
       "\n",
       "                           CountryLabel  \n",
       "EndTime                                  \n",
       "2022-01-01 00:00:00+00:00             2  \n",
       "2022-01-01 01:00:00+00:00             2  \n",
       "2022-01-01 02:00:00+00:00             2  \n",
       "2022-01-01 03:00:00+00:00             2  \n",
       "2022-01-01 04:00:00+00:00             2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9812b90fba45dfa947c705ede7b02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablo\\.conda\\envs\\python311\\Lib\\site-packages\\ydata_profiling\\model\\correlations.py:66: UserWarning: There was an attempt to calculate the auto correlation, but this failed.\n",
      "To hide this warning, disable the calculation\n",
      "(using `df.profile_report(correlations={\"auto\": {\"calculate\": False}})`\n",
      "If this is problematic for your use case, please report this as an issue:\n",
      "https://github.com/ydataai/ydata-profiling/issues\n",
      "(include the error message: 'cannot reindex on an axis with duplicate labels')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c5a888008f457692fe67f6eabb0a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1878e6476b3041d5bb4ff56e489ca69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render widgets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924cfcdc92ef41dbbf188afb6c449aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25982d73f88e4717a199808b370b4b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9b90716dd34e1cb327241b564b3099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the data profiling\n",
    "profile = ProfileReport(eda_df, title=\"Profiling Report\")\n",
    "\n",
    "profile.to_widgets() # Displaying the profile in the Jupyter notebook\n",
    "profile.to_file(\"eda_df_profiling_output.html\") # Saving the profiling in an html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          country_code  quantity_sum   Load  surplus  \\\n",
      "EndTime                                                                \n",
      "2022-03-05 07:00:00+00:00           DK        1687.0   4225  -2538.0   \n",
      "2022-05-03 17:00:00+00:00           DK        1278.0   4189  -2911.0   \n",
      "2022-05-08 03:00:00+00:00           DK        2006.0   2763   -757.0   \n",
      "2022-09-19 01:00:00+00:00           DK        2006.0   2763   -757.0   \n",
      "2022-09-22 06:00:00+00:00           DK        1278.0   4189  -2911.0   \n",
      "2022-09-28 04:00:00+00:00           DK        1235.0   3616  -2381.0   \n",
      "2022-10-28 05:00:00+00:00           DK        1687.0   4225  -2538.0   \n",
      "2022-11-13 13:00:00+00:00           DK        1235.0   3616  -2381.0   \n",
      "2022-06-06 22:00:00+00:00           HU         612.0  15062 -14450.0   \n",
      "2022-12-24 23:00:00+00:00           HU         612.0  15062 -14450.0   \n",
      "2022-01-02 22:00:00+00:00           SP        7700.0  24535 -16835.0   \n",
      "2022-04-28 04:00:00+00:00           SP        7700.0  24535 -16835.0   \n",
      "2022-07-26 02:00:00+00:00           SE        8917.0  10212  -1295.0   \n",
      "2022-10-15 02:00:00+00:00           SE        8917.0  10212  -1295.0   \n",
      "2022-02-08 13:00:00+00:00           UK        1090.0   2264  -1174.0   \n",
      "2022-02-08 16:00:00+00:00           UK        1090.0   2264  -1174.0   \n",
      "\n",
      "                           CountryLabel  \n",
      "EndTime                                  \n",
      "2022-03-05 07:00:00+00:00             3  \n",
      "2022-05-03 17:00:00+00:00             3  \n",
      "2022-05-08 03:00:00+00:00             3  \n",
      "2022-09-19 01:00:00+00:00             3  \n",
      "2022-09-22 06:00:00+00:00             3  \n",
      "2022-09-28 04:00:00+00:00             3  \n",
      "2022-10-28 05:00:00+00:00             3  \n",
      "2022-11-13 13:00:00+00:00             3  \n",
      "2022-06-06 22:00:00+00:00             5  \n",
      "2022-12-24 23:00:00+00:00             5  \n",
      "2022-01-02 22:00:00+00:00             0  \n",
      "2022-04-28 04:00:00+00:00             0  \n",
      "2022-07-26 02:00:00+00:00             4  \n",
      "2022-10-15 02:00:00+00:00             4  \n",
      "2022-02-08 13:00:00+00:00             1  \n",
      "2022-02-08 16:00:00+00:00             1  \n"
     ]
    }
   ],
   "source": [
    "# Finding duplicate rows\n",
    "duplicates = eda_df.duplicated(keep=False) # 'keep=False' marks all duplicates as True\n",
    "\n",
    "# Displaying duplicate rows\n",
    "duplicate_rows = eda_df[duplicates]\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = eda_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [country_code, quantity_sum, Load, surplus, CountryLabel]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Finding duplicate rows\n",
    "duplicates = eda_df.duplicated(keep=False) # 'keep=False' marks all duplicates as True\n",
    "\n",
    "# Displaying duplicate rows\n",
    "duplicate_rows = eda_df[duplicates]\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 67881 entries, 2022-01-01 00:00:00+00:00 to 2022-12-31 23:00:00+00:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   country_code  67881 non-null  object \n",
      " 1   quantity_sum  67881 non-null  float64\n",
      " 2   Load          67881 non-null  int64  \n",
      " 3   surplus       67881 non-null  float64\n",
      " 4   CountryLabel  67881 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(1)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "eda_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df.to_csv('eda_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_df = eda_df[eda_df['country_code'] == 'SP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_up_val = (arima_df['surplus'].min() * -1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_df['surplus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_df.loc[:, 'surplus'] = arima_df['surplus'] + eda_up_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_df[arima_df['surplus'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la columna 'quantity'\n",
    "plt.plot(arima_df.index, arima_df['surplus'])\n",
    "\n",
    "# Añadir etiquetas y título\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Surplus')\n",
    "plt.title('Time Series for \"Surplus\" Variable for SP')\n",
    "\n",
    "# Mostrar la leyenda si es necesario\n",
    "# plt.legend(['quantity'])\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1  # replace with actual\n",
    "d = 1  # replace with actual\n",
    "q = 1  # replace with actual\n",
    "\n",
    "# Fit the ARIMA model (using the known part of the time series)\n",
    "model = ARIMA(arima_df['surplus'], order=(p, d, q))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast the missing values\n",
    "# The 'steps' argument would be the number of hours in the missing months\n",
    "forecast = model_fit.get_forecast(steps=4350) #hours in a monyh\n",
    "\n",
    "# The forecast object contains the predicted values and other information\n",
    "forecasted_values = forecast.predicted_mean\n",
    "\n",
    "# You may also want to extract the confidence intervals of the forecasts\n",
    "conf_int = forecast.conf_int()\n",
    "\n",
    "# Do something with the forecasted values, like plotting them or inserting them back into your DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose appropriate parameters for an ARIMA model, you can follow these steps:\n",
    "\n",
    "**1. Stationarity Check**\n",
    "\n",
    "First, check if your time series is stationary. This can be done using a statistical test, such as the Augmented Dickey-Fuller (ADF) test.\n",
    "\n",
    "**2. Differencing**\n",
    "\n",
    "If the time series is not stationary, apply differencing (d parameter in ARIMA) to make it stationary.\n",
    "\n",
    "3. Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)\n",
    "Once you have a stationary series, plot the ACF and PACF. These plots will help you determine the p and q parameters.\n",
    "\n",
    "ACF: Helps to identify the Moving Average (MA) component, which corresponds to q.\n",
    "PACF: Helps to identify the Autoregressive (AR) component, which corresponds to p.\n",
    "4. Model Identification\n",
    "Based on the ACF and PACF plots, you can identify initial values for p and q:\n",
    "\n",
    "The lag where the PACF cuts off is the estimated p.\n",
    "The lag where the ACF cuts off is the estimated q.\n",
    "5. Iterative Process\n",
    "Model selection is an iterative process. You may need to try different combinations of p, d, and q and use model selection criteria like AIC (Akaike Information Criterion) to find the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Check for Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for stationarity\n",
    "result = adfuller(arima_df['surplus'])\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "# If p-value > 0.05, the series is not stationary and you need to difference it\n",
    "# The null hypothesis that the series has a unit root (i.e., is non-stationary) cannot be rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Augmented Dickey-Fuller (ADF) test is a common statistical test used to determine whether a given time series is stationary or not. The interpretation of the values given from an ADF test is as follows:\n",
    "\n",
    "1. **ADF Statistic**: This is the test statistic used to reject or fail to reject the null hypothesis. The more negative the statistic, the stronger the rejection of the null hypothesis that the series has a unit root (i.e., is non-stationary).\n",
    "\n",
    "   - In this case, the ADF Statistic is `-4.083998`. Since it's a negative value and depending on the critical values for the ADF test at different confidence levels (usually -3.5, -2.9, and -2.6 for the 1%, 5%, and 10% levels), a value of `-4.083998` is quite negative, which suggests that the null hypothesis can be rejected. This means the series is likely stationary.\n",
    "\n",
    "2. **p-value**: This indicates the probability of observing the test results under the null hypothesis. In other words, it's the probability of finding the observed (or more extreme) results when the null hypothesis is true.\n",
    "\n",
    "   - With a p-value of `0.001029`, which is less than 0.05 (a common threshold for statistical significance), you can reject the null hypothesis with high confidence. This low p-value indicates strong evidence against the null hypothesis, suggesting that the time series does not have a unit root and is stationary.\n",
    "\n",
    "In conclusion, both the ADF statistic and the p-value indicate that the time series is stationary, and no differencing is needed if you are using this series to build an ARIMA model. It's always good practice to also compare the ADF statistic to the critical values provided in the test output to confirm this conclusion at different confidence levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Applying Differencing (or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the series is non-stationary (p-value > 0.05), the code then sets d = 1, which indicates that the series will be differenced once. The .diff(periods=d) function calculates the difference between each data point and the one d periods before it. The .dropna() part ensures that any NaN values created by the differencing (because the first d periods don't have d prior data points to subtract from) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply differencing if needed and determine 'd'\n",
    "if result[1] > 0.05:\n",
    "    # Assuming one level of differencing makes the series stationary\n",
    "    d = 1\n",
    "    arima_df.loc[:, 'surplus_diff'] = arima_df['surplus'].diff(periods=d).dropna()\n",
    "else:\n",
    "    d = 0\n",
    "    arima_df.loc[:, 'surplus_diff'] = arima_df['surplus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying differencing to a time series to achieve stationarity, plotting the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) is crucial for:\n",
    "\n",
    "1. **Identification of ARIMA Model Parameters**:\n",
    "   - **AR (p)**: The PACF plot is used to identify the order of the Autoregressive (AR) part of an ARIMA model. The AR part captures the relationship between an observation and a certain number of lagged observations. The PACF plot shows the partial correlation of a stationarized series with its own lagged values, controlling for the values of the time series at all shorter lags. The point at which the PACF cuts off (the lag after which most PACF values are not significantly different from zero) is the suggested order of the AR component.\n",
    "   \n",
    "   - **MA (q)**: The ACF plot is used to identify the order of the Moving Average (MA) part of the ARIMA model. The MA part models the relationship between an observation and a residual error from a moving average model applied to lagged observations. The ACF plot shows the correlation of the series with its lags. Similar to the PACF, the point at which the ACF plot cuts off is the suggested order of the MA component.\n",
    "\n",
    "2. **Insight into the Data**:\n",
    "   - The ACF and PACF plots provide a visual insight into how the data is related to itself over time. For instance, if the ACF shows a slow decay, this indicates a long-term relationship in the data, which an ARIMA model might not capture without an appropriate differencing order (d).\n",
    "\n",
    "3. **Model Diagnostics**:\n",
    "   - After fitting the model, the ACF and PACF plots can be used for diagnostics to check for any autocorrelation in the residuals of the model. If there is significant autocorrelation in the residuals, the model may not be adequately capturing the structure of the time series, and further adjustments may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF and PACF\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "plot_acf(arima_df['surplus_diff'], lags=1800, ax=ax1)  # Change lags as needed\n",
    "plot_pacf(arima_df['surplus_diff'], lags=180, ax=ax2)  # Change lags as needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ACF plot with an increased number of lags shows the following characteristics:\n",
    "\n",
    "Gradual Decline: The ACF starts at 1 (as it always will for lag 0, because it's the correlation of the series with itself) and shows a very slow and gradual decline as the lags increase.\n",
    "\n",
    "No Sharp Cutoff: There is no sharp cutoff point in the autocorrelation. Instead, it slowly diminishes, which suggests that the series may have a long memory. In other words, the current value of the series is likely influenced by many past values, not just recent ones.\n",
    "\n",
    "Stationarity Concern: The fact that autocorrelation remains positive and slowly decays as lags increase may indicate that the time series is not stationary, even after differencing. This is often the case in time series with strong trend or seasonal components that have not been fully accounted for.\n",
    "\n",
    "Model Implication: Such an ACF plot suggests that a simple ARIMA model might not be sufficient for capturing the underlying process of the time series. Instead, you might need to consider additional differencing to remove trends or seasonality, or even use models that can capture seasonality (like SARIMA) or trends (like ARIMA with a drift term) more explicitly.\n",
    "\n",
    "Data Transformation: Before deciding on a model, further transformations or decompositions could be considered to better understand and model the underlying pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Additional Processing Based on Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform seasonal decomposition\n",
    "# The period parameter should be set according to the known or suspected seasonality in your data\n",
    "# For example, if you suspect annual seasonality in daily data, the period would be 365.\n",
    "# For monthly data with annual seasonality, it would be 12.\n",
    "result = seasonal_decompose(arima_df['surplus_diff'], model='additive', period=365)\n",
    "\n",
    "# Plot the seasonal decomposition with an increased figure size\n",
    "fig = result.plot()\n",
    "fig.set_size_inches(10, 8)  # You can adjust the size as needed (width, height in inches)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detrending\n",
    "arima_df.loc[:, 'detrended'] = arima_df['surplus_diff'] - result.trend\n",
    "\n",
    "# Seasonal Adjustment\n",
    "arima_df.loc[:, 'seasonally_adjusted'] = arima_df['surplus_diff'] - result.seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = ARIMA(arima_df['detrended'], order=(5,1,0)) # These parameters (p,d,q) can be tuned\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast = model_fit.forecast(steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detrended data\n",
    "plt.plot(arima_df_log.index, arima_df_log, label='Detrended Data')\n",
    "#plt.set_title('Detrended Time Series')\n",
    "#plt.set_ylabel('Value')\n",
    "#plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_log_day = arima_df_log.resample('H').ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detrended data\n",
    "plt.plot(arima_log_day.index, arima_log_day, label='Detrended Data')\n",
    "#plt.set_title('Detrended Time Series')\n",
    "#plt.set_ylabel('Value')\n",
    "#plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detrending\n",
    "arima_log_day.loc[:, 'detrended'] = arima_df['surplus_diff'] - result.trend\n",
    "\n",
    "# Seasonal Adjustment\n",
    "arima_log_day.loc[:, 'seasonally_adjusted'] = arima_df['surplus_diff'] - result.seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Detrended data\n",
    "axs[0].plot(arima_log_day.index, arima_log_day['detrended'], label='Detrended Data')\n",
    "axs[0].set_title('Detrended Time Series')\n",
    "axs[0].set_ylabel('Value')\n",
    "axs[0].legend()\n",
    "\n",
    "# Seasonally adjusted data\n",
    "axs[1].plot(arima_df.index, arima_df['seasonally_adjusted'], label='Seasonally Adjusted Data', color='orange')\n",
    "axs[1].set_title('Seasonally Adjusted Time Series')\n",
    "axs[1].set_ylabel('Value')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detrended Time Series (Top, Blue Plot)**:\n",
    "- The detrended series has had the trend component removed.\n",
    "- It fluctuates around a mean that appears to be close to zero, which is what you would expect after removing a trend.\n",
    "- The variability in the series doesn't seem to diminish or increase over time, which is good as it suggests that the variance is relatively stable after detrending.\n",
    "- You can now analyze this detrended series for cyclic patterns that are not part of the longer trend.\n",
    "\n",
    "**Seasonally Adjusted Time Series (Bottom, Orange Plot)**:\n",
    "- The seasonally adjusted series has had the seasonal component removed.\n",
    "- The fact that it looks similar to the original series might suggest that the seasonal component was not very strong, or that the period used for seasonal decomposition was not correctly specified.\n",
    "- If the seasonally adjusted series looks identical to the original series, it might indicate that the `seasonal_decompose` function did not properly identify and subtract the seasonal component from the original series.\n",
    "\n",
    "The key here is understanding the period used for decomposition. The period parameter should correspond to the length of the seasonal cycle. For example, if the data is monthly and the seasonality is expected to repeat annually, the period should be 12. If the data is daily and you expect an annual cycle, then the period should be around 365.\n",
    "\n",
    "However, since the data is hourly, an annual period of 365 would not be appropriate. Instead, the period should match the expected seasonal cycle in hours. For instance, if there's a daily pattern, the period might be 24 (for the number of hours in a day).\n",
    "\n",
    "It's also possible that the seasonality in your data is not captured by additive decomposition due to the nature of the seasonal pattern. If the seasonality increases or decreases with the level of the time series, a multiplicative model might be more appropriate.\n",
    "\n",
    "To address these points, you may need to:\n",
    "\n",
    "1. Revisit the period used for seasonal decomposition and ensure it matches the expected seasonal cycle in your data.\n",
    "2. Consider whether a multiplicative model would be more appropriate if the seasonality is proportional to the level of the time series (not an option since we have negative values).\n",
    "4. Investigate alternative methods of seasonal adjustment if `seasonal_decompose` does not seem to be capturing the seasonality effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data is sampled hourly and the seasonal pattern repeats twice per month (from the plot), we would need to calculate the period in terms of hours. Given that a month on average has about 30.44 days, and you're observing a pattern that repeats twice within that period, you can calculate the period as follows:\n",
    "\n",
    "\\[ \\text{Period in hours} = \\frac{\\text{Average number of days in a month} \\times 24 \\text{ hours}}{\\text{Number of cycles per month}} \\]\n",
    "\n",
    "\\[ \\text{Period in hours} = \\frac{30.44 \\times 24}{2} \\]\n",
    "\n",
    "Let's calculate that.\n",
    "\n",
    "The correct value for the period, given that the seasonal pattern repeats twice per month, would be approximately 365.28 hours. When specifying the period for seasonal decomposition, you should round this to the nearest whole number. Therefore, you can use a period of 365 hours for the `seasonal_decompose` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of days in a month\n",
    "average_days_per_month = 30.44\n",
    "\n",
    "# Number of cycles per month\n",
    "cycles_per_month = 2\n",
    "\n",
    "# Calculate the period in hours\n",
    "period_in_hours = (average_days_per_month * 24) / cycles_per_month\n",
    "period_in_hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you can run a SARIMA model using the detrended time series. Seasonal ARIMA, or SARIMA, is specifically designed to handle both seasonality and non-seasonal factors in time series data. If the seasonal decomposition did not adequately capture the seasonality, but you have identified a seasonal pattern (as you mentioned, repeating twice per month), you can incorporate this knowledge directly into the SARIMA model parameters.\n",
    "\n",
    "Here's a general approach to fitting a SARIMA model:\n",
    "\n",
    "Selecting Model Orders: You need to choose the SARIMA model orders. These include the non-seasonal parameters (p, d, q) and the seasonal parameters (P, D, Q, s). Since you've detrended the data, d might be 0 or 1 depending on if the data needs further differencing to achieve stationarity. The seasonal parameters will incorporate the seasonality you've observed.\n",
    "\n",
    "Seasonal Period (s): This is the seasonality of the data. In your case, if the seasonality is twice per month, you've already calculated the seasonal period to be approximately 365 hours.\n",
    "\n",
    "Parameter Estimation: Use the ACF and PACF plots of the detrended data to estimate the initial SARIMA parameters. You can also use grid search methods to test different combinations of parameters and select the best model based on some criteria like the AIC (Akaike Information Criterion).\n",
    "\n",
    "Model Diagnostics: After fitting the model, check the diagnostics to ensure that the residuals of your model are white noise (no autocorrelation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model order (these are arbitrary and should be determined through analysis)\n",
    "p, d, q = 1, 0, 1  # Non-seasonal orders\n",
    "P, D, Q, s = 1, 0, 1, 365  # Seasonal orders (seasonal period s is 365 hours)\n",
    "\n",
    "# Fit the SARIMA model\n",
    "sarima_model = SARIMAX(arima_df['detrended'], order=(p, d, q), seasonal_order=(P, D, Q, s))\n",
    "sarima_results = sarima_model.fit()\n",
    "\n",
    "# Diagnostics and validation\n",
    "print(sarima_results.summary())\n",
    "sarima_results.plot_diagnostics(figsize=(10, 8))\n",
    "plt.show()\n",
    "\n",
    "# Predictions or forecasting\n",
    "sarima_forecast = sarima_results.get_forecast(steps=48)  # Forecasting the next 48 hours\n",
    "forecast_values = sarima_forecast.predicted_mean\n",
    "confidence_intervals = sarima_forecast.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
