{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEN Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where raw CSV files are stored\n",
    "directory = \"../jupyter_notebook/data_samples\"\n",
    "# Parsing date strings, ignoring any timezone information and converting them to datetime objects\n",
    "date_parser = lambda x: pd.to_datetime(x[:22])\n",
    "# List to hold all the dataframes\n",
    "dataframes = []\n",
    "dict_of_dfs = {}\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    if re.match(r'gen_[A-Z]{2}_[A-Z0-9]+\\.csv', filename):\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(directory, filename), converters={'EndTime': date_parser}).set_index('EndTime')\n",
    "       \n",
    "        # Extract country and energy type from filename\n",
    "        _, country, energy_type = filename.split('_')\n",
    "        energy_type = energy_type.replace('.csv', '') # Remove the file extension\n",
    "\n",
    "        # Add country and energy type as new columns\n",
    "        df['CountryCode'] = country\n",
    "        df['EnergyTypeCode'] = energy_type\n",
    "        \n",
    "        #Dropeamos valores duplicados que no tienen AREA ID.\n",
    "        df.dropna(subset=['AreaID'],inplace=True)\n",
    "        \n",
    "        #Dropeamos el StartTime que no usaremos\n",
    "        df.drop(columns=['StartTime'],inplace=True)\n",
    "        \n",
    "        #Reesampleamos\n",
    "        df=df.resample('30T').sum()\n",
    "        \n",
    "        #Cambio de dato\n",
    "        df['AreaID']=df['AreaID'].astype(str)\n",
    "        \n",
    "        #A los no existentes le ponemos none\n",
    "        #df.loc[df['AreaID']=='0','quantity']=None\n",
    "        \n",
    "        #Ponemos como columna al EndTime\n",
    "        df.reset_index(inplace=True)\n",
    "        \n",
    "        # Calcula la diferencia de tiempo entre filas consecutivas en minutos\n",
    "        df['TimeDifference'] = df['EndTime'].diff().dt.total_seconds() / 60\n",
    "\n",
    "        # Divide 60 por la diferencia de tiempo y guarda el resultado en una nueva variable\n",
    "        valor = 60 / df['TimeDifference'][1]\n",
    "        \n",
    "        # Creamos una nueva columna 'grupo' que representa grupos consecutivos cada dos registros\n",
    "        df['grupo'] = (df.index // valor) + 1\n",
    "\n",
    "        # Iteramos sobre cada grupo para imputar NaN con la media del grupo\n",
    "        for grupo, data_grupo in df.groupby('grupo'):\n",
    "            if data_grupo['quantity'].isnull().all():\n",
    "                continue  \n",
    "            \n",
    "            # Calculamos la media del grupo excluyendo NaN\n",
    "            media_grupo = data_grupo['quantity'].mean(skipna=True)\n",
    "\n",
    "            # Imputamos NaN con la media del grupo\n",
    "            df.loc[df['grupo'] == grupo, 'quantity'] = df.loc[df['grupo'] == grupo, 'quantity'].fillna(media_grupo)\n",
    "        \n",
    "        #Dropeamos duplicados luego de imputar.\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        #For resampling\n",
    "        df.set_index('EndTime',inplace=True)\n",
    "        \n",
    "        numeric_cols = df.select_dtypes(include=['number'])\n",
    "        categorical_cols = df.select_dtypes(exclude=['number', 'datetime64[ns]', 'bool'])\n",
    "        \n",
    "        # Resample the numeric columns and sum\n",
    "        resampled_df_num = numeric_cols.resample('H').sum()\n",
    "\n",
    "        # Resample the categorical columns.\n",
    "        # Here, we take the first value. Adjust the method if needed (e.g., 'last', or a custom function to get the mode)\n",
    "        resampled_df_cat= categorical_cols.resample('H').last()\n",
    "\n",
    "        # Combine the resampled DataFrames back together\n",
    "        resampled_df = pd.concat([resampled_df_num, resampled_df_cat], axis=1)\n",
    "        \n",
    "        #Dropeamos vacios luego de resamplear.\n",
    "        resampled_df.dropna(inplace=True)\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        #dataframes.append(resampled_df)\n",
    "        \n",
    "        dict_of_dfs[f'{country}_{energy_type}']=resampled_df\n",
    "\n",
    "# Concatenate all dataframes (if needed)\n",
    "#final_df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>TimeDifference</th>\n",
       "      <th>grupo</th>\n",
       "      <th>AreaID</th>\n",
       "      <th>UnitName</th>\n",
       "      <th>PsrType</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>EnergyTypeCode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EndTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00+00:00</th>\n",
       "      <td>8642</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10Y1001A1001A83F10Y1001A1001A83F</td>\n",
       "      <td>MAWMAW</td>\n",
       "      <td>B01B01</td>\n",
       "      <td>DEDE</td>\n",
       "      <td>B01B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00+00:00</th>\n",
       "      <td>17316</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10Y1001A1001A83F10Y1001A1001A83F</td>\n",
       "      <td>MAWMAW</td>\n",
       "      <td>B01B01</td>\n",
       "      <td>DEDE</td>\n",
       "      <td>B01B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00+00:00</th>\n",
       "      <td>17342</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10Y1001A1001A83F10Y1001A1001A83F</td>\n",
       "      <td>MAWMAW</td>\n",
       "      <td>B01B01</td>\n",
       "      <td>DEDE</td>\n",
       "      <td>B01B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00+00:00</th>\n",
       "      <td>17350</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10Y1001A1001A83F10Y1001A1001A83F</td>\n",
       "      <td>MAWMAW</td>\n",
       "      <td>B01B01</td>\n",
       "      <td>DEDE</td>\n",
       "      <td>B01B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00+00:00</th>\n",
       "      <td>17375</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10Y1001A1001A83F10Y1001A1001A83F</td>\n",
       "      <td>MAWMAW</td>\n",
       "      <td>B01B01</td>\n",
       "      <td>DEDE</td>\n",
       "      <td>B01B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 19:00:00+00:00</th>\n",
       "      <td>18513</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17512.0</td>\n",
       "      <td>10Y1001A1001A83F10Y1001A1001A83F</td>\n",
       "      <td>MAWMAW</td>\n",
       "      <td>B01B01</td>\n",
       "      <td>DEDE</td>\n",
       "      <td>B01B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 20:00:00+00:00</th>\n",
       "      <td>18121</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17514.0</td>\n",
       "      <td>10Y1001A1001A83F10Y1001A1001A83F</td>\n",
       "      <td>MAWMAW</td>\n",
       "      <td>B01B01</td>\n",
       "      <td>DEDE</td>\n",
       "      <td>B01B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 21:00:00+00:00</th>\n",
       "      <td>17916</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17516.0</td>\n",
       "      <td>10Y1001A1001A83F10Y1001A1001A83F</td>\n",
       "      <td>MAWMAW</td>\n",
       "      <td>B01B01</td>\n",
       "      <td>DEDE</td>\n",
       "      <td>B01B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 22:00:00+00:00</th>\n",
       "      <td>17770</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17518.0</td>\n",
       "      <td>10Y1001A1001A83F10Y1001A1001A83F</td>\n",
       "      <td>MAWMAW</td>\n",
       "      <td>B01B01</td>\n",
       "      <td>DEDE</td>\n",
       "      <td>B01B01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 23:00:00+00:00</th>\n",
       "      <td>17460</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17520.0</td>\n",
       "      <td>10Y1001A1001A83F10Y1001A1001A83F</td>\n",
       "      <td>MAWMAW</td>\n",
       "      <td>B01B01</td>\n",
       "      <td>DEDE</td>\n",
       "      <td>B01B01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           quantity  TimeDifference    grupo  \\\n",
       "EndTime                                                        \n",
       "2022-01-01 00:00:00+00:00      8642            30.0      1.0   \n",
       "2022-01-01 01:00:00+00:00     17316            60.0      4.0   \n",
       "2022-01-01 02:00:00+00:00     17342            60.0      6.0   \n",
       "2022-01-01 03:00:00+00:00     17350            60.0      8.0   \n",
       "2022-01-01 04:00:00+00:00     17375            60.0     10.0   \n",
       "...                             ...             ...      ...   \n",
       "2022-12-31 19:00:00+00:00     18513            60.0  17512.0   \n",
       "2022-12-31 20:00:00+00:00     18121            60.0  17514.0   \n",
       "2022-12-31 21:00:00+00:00     17916            60.0  17516.0   \n",
       "2022-12-31 22:00:00+00:00     17770            60.0  17518.0   \n",
       "2022-12-31 23:00:00+00:00     17460            60.0  17520.0   \n",
       "\n",
       "                                                     AreaID UnitName PsrType  \\\n",
       "EndTime                                                                        \n",
       "2022-01-01 00:00:00+00:00  10Y1001A1001A83F10Y1001A1001A83F   MAWMAW  B01B01   \n",
       "2022-01-01 01:00:00+00:00  10Y1001A1001A83F10Y1001A1001A83F   MAWMAW  B01B01   \n",
       "2022-01-01 02:00:00+00:00  10Y1001A1001A83F10Y1001A1001A83F   MAWMAW  B01B01   \n",
       "2022-01-01 03:00:00+00:00  10Y1001A1001A83F10Y1001A1001A83F   MAWMAW  B01B01   \n",
       "2022-01-01 04:00:00+00:00  10Y1001A1001A83F10Y1001A1001A83F   MAWMAW  B01B01   \n",
       "...                                                     ...      ...     ...   \n",
       "2022-12-31 19:00:00+00:00  10Y1001A1001A83F10Y1001A1001A83F   MAWMAW  B01B01   \n",
       "2022-12-31 20:00:00+00:00  10Y1001A1001A83F10Y1001A1001A83F   MAWMAW  B01B01   \n",
       "2022-12-31 21:00:00+00:00  10Y1001A1001A83F10Y1001A1001A83F   MAWMAW  B01B01   \n",
       "2022-12-31 22:00:00+00:00  10Y1001A1001A83F10Y1001A1001A83F   MAWMAW  B01B01   \n",
       "2022-12-31 23:00:00+00:00  10Y1001A1001A83F10Y1001A1001A83F   MAWMAW  B01B01   \n",
       "\n",
       "                          CountryCode EnergyTypeCode  \n",
       "EndTime                                               \n",
       "2022-01-01 00:00:00+00:00        DEDE         B01B01  \n",
       "2022-01-01 01:00:00+00:00        DEDE         B01B01  \n",
       "2022-01-01 02:00:00+00:00        DEDE         B01B01  \n",
       "2022-01-01 03:00:00+00:00        DEDE         B01B01  \n",
       "2022-01-01 04:00:00+00:00        DEDE         B01B01  \n",
       "...                               ...            ...  \n",
       "2022-12-31 19:00:00+00:00        DEDE         B01B01  \n",
       "2022-12-31 20:00:00+00:00        DEDE         B01B01  \n",
       "2022-12-31 21:00:00+00:00        DEDE         B01B01  \n",
       "2022-12-31 22:00:00+00:00        DEDE         B01B01  \n",
       "2022-12-31 23:00:00+00:00        DEDE         B01B01  \n",
       "\n",
       "[8760 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_dfs['DE_B01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paises = [\"SP\", \"UK\", \"DE\", \"DK\", \"HU\", \"SE\", \"IT\", \"PO\", \"NE\"]\n",
    "\n",
    "new_dict_of_dfs = {}\n",
    "\n",
    "for pais in paises:\n",
    "    # Crear una nueva clave con el formato \"dic_[pais]\"\n",
    "    nueva_clave = f\"dic_{pais}\"\n",
    "\n",
    "    # Filtrar DataFrames que contienen el código de país en la clave\n",
    "    dataframes_filtrados = {key: df for key, df in dict_of_dfs.items() if pais in key}\n",
    "\n",
    "    # Asignar el nuevo diccionario a la nueva clave\n",
    "    new_dict_of_dfs[nueva_clave] = dataframes_filtrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output={}\n",
    "for name,dataf in new_dict_of_dfs.items():\n",
    "    concat_df = pd.concat(dataf)\n",
    "    output[name]=concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1={}\n",
    "for name,dataf in output.items():\n",
    "    suma_grupo = dataf.groupby('EndTime')['quantity'].sum().reset_index()\n",
    "    output_1[name]=suma_grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(output_1), 1, figsize=(10, 6 * len(output_1)))\n",
    "\n",
    "# Iterar sobre cada DataFrame en el diccionario\n",
    "for i, (nombre, df) in enumerate(output_1.items()):\n",
    "    axs[i].plot(df['EndTime'], df['quantity'], label=nombre)\n",
    "    axs[i].set_title(nombre)\n",
    "    axs[i].set_xlabel('EndTime')\n",
    "    axs[i].set_ylabel('quantity')\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where raw CSV files are stored\n",
    "directory = \"../jupyter_notebook/data_samples\"\n",
    "# Parsing date strings, ignoring any timezone information and converting them to datetime objects\n",
    "date_parser = lambda x: pd.to_datetime(x[:22])\n",
    "# List to hold all the dataframes\n",
    "dataframes = []\n",
    "dict_of_dfs_load = {}\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    if re.match(r'load_[A-Z]{2}+\\.csv', filename):\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(directory, filename), converters={'EndTime': date_parser}).set_index('EndTime')\n",
    "        \n",
    "        _, country = filename.split('_')\n",
    "        \n",
    "        country = country.replace('.csv', '') # Remove the file extension\n",
    "        \n",
    "        country = 'dic_'+country\n",
    "        \n",
    "        dict_of_dfs_load[country]=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs_load['dic_SP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(dict_of_dfs_load), 1, figsize=(10, 6 * len(dict_of_dfs_load)))\n",
    "\n",
    "# Iterar sobre cada DataFrame en el diccionario\n",
    "for i, (nombre, df) in enumerate(dict_of_dfs_load.items()):\n",
    "    axs[i].plot(df.index, df['Load'], label=nombre)\n",
    "    axs[i].set_title(nombre)\n",
    "    axs[i].set_xlabel('EndTime')\n",
    "    axs[i].set_ylabel('quantity')\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEN and Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gen=output_1.copy() #Dic Dataframes with gen energy\n",
    "dict_load=dict_of_dfs_load #Dic Dataframes with load energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1  # replace with actual\n",
    "d = 1  # replace with actual\n",
    "q = 1  # replace with actual\n",
    "\n",
    "# Fit the ARIMA model (using the known part of the time series)\n",
    "model = ARIMA(df['quantity'], order=(p, d, q))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast the missing values\n",
    "# The 'steps' argument would be the number of hours in the missing months\n",
    "forecast = model_fit.get_forecast(steps=4350) #hours in a monyh\n",
    "\n",
    "# The forecast object contains the predicted values and other information\n",
    "forecasted_values = forecast.predicted_mean\n",
    "\n",
    "# You may also want to extract the confidence intervals of the forecasts\n",
    "conf_int = forecast.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose appropriate parameters for an ARIMA model, you can follow these steps:\n",
    "\n",
    "1. Stationarity Check\n",
    "First, check if your time series is stationary. This can be done using a statistical test, such as the Augmented Dickey-Fuller (ADF) test.\n",
    "\n",
    "2. Differencing\n",
    "If the time series is not stationary, apply differencing (d parameter in ARIMA) to make it stationary.\n",
    "\n",
    "3. Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)\n",
    "Once you have a stationary series, plot the ACF and PACF. These plots will help you determine the p and q parameters.\n",
    "\n",
    "ACF: Helps to identify the Moving Average (MA) component, which corresponds to q.\n",
    "PACF: Helps to identify the Autoregressive (AR) component, which corresponds to p.\n",
    "4. Model Identification\n",
    "Based on the ACF and PACF plots, you can identify initial values for p and q:\n",
    "\n",
    "The lag where the PACF cuts off is the estimated p.\n",
    "The lag where the ACF cuts off is the estimated q.\n",
    "5. Iterative Process\n",
    "Model selection is an iterative process. You may need to try different combinations of p, d, and q and use model selection criteria like AIC (Akaike Information Criterion) to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "# Assuming 'df' is your DataFrame with 'quantity' after loading your data\n",
    "\n",
    "# Check for stationarity\n",
    "result = adfuller(df['quantity'])\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "# If p-value > 0.05, the series is not stationary and you need to difference it\n",
    "\n",
    "# Apply differencing if needed and determine 'd'\n",
    "if result[1] > 0.05:\n",
    "    # Assuming one level of differencing makes the series stationary\n",
    "    d = 1\n",
    "    df['quantity_diff'] = df['quantity'].diff(periods=d).dropna()\n",
    "else:\n",
    "    d = 0\n",
    "    df['quantity_diff'] = df['quantity']\n",
    "\n",
    "# Plot ACF and PACF\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "plot_acf(df['quantity_diff'], lags=40, ax=ax1)  # Change lags as needed\n",
    "plot_pacf(df['quantity_diff'], lags=40, ax=ax2)  # Change lags as needed\n",
    "plt.show()\n",
    "\n",
    "# Look at where the plots cross the significance level for the first time to estimate 'p' and 'q'\n",
    "# These are initial estimates and might need to be refined\n",
    "\n",
    "# For example, if PACF cuts off after lag 2, and ACF tails off slowly, you might start with:\n",
    "p = 2\n",
    "q = 2\n",
    "\n",
    "# Try fitting an ARIMA model with the initial parameters\n",
    "model = sm.tsa.arima.ARIMA(df['quantity'], order=(p, d, q))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Evaluate model - check the AIC and the residual plots\n",
    "print(model_fit.summary())\n",
    "model_fit.plot_diagnostics(figsize=(15, 8))\n",
    "plt.show()\n",
    "\n",
    "# You may need to refine your (p, d, q) based on the diagnostics and AIC values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
